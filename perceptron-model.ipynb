{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jonathan Rossi**  \n",
    "*April 9, 2016*  \n",
    "Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning Algorithm for Classification #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the Perceptron Learning Algorithm for classification, designed by Frank Rosenblatt (F. Rosenblatt, 'The Perceptron, a Perceiving and Recognizing Automaton,' Cornell Aeronautical Laboratory, 1957). As this is a learning experience for me, this implementation draws heavily on Sebastian Raschka's code in his book 'Python Machine Learning' (Packt Publishing, 2015). Thanks for a rock-solid explanation of this algorithm, Sebastian!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Design Overview\n",
    "* Implementation\n",
    "* Implementation Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Overview ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, the algorithm finds the ideal weights (aka, coefficients) to apply to each feature in the data set, so that the dot product of the feature vector and the weights vector falls above or below a threshold, which predicts whether the observation (i.e., sample) is, or is not, of a particular class. The algorithm initializes the weights vector to the zero-vector, or a vector of small arbitrary numbers, then takes one observation at a time and, for each observation, computes the output value (i.e., the dot product of the weights vector and the feature vector), then updates the weights vector based on the difference between the ground truth value and the output value. For correct predictions, the weights vector is left unchanged. For incorrect predictions, individual values in the weights vector are adjusted to be more positive or more negative—in proportion to the magnitude of the corresponding feature vector—so as to \"push\" the prediction closer to the ground truth value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Perceptron model class.\n",
    "class Perceptron(object):\n",
    "    def __init__(self, learnrate = 0.01, n_iter = 10):\n",
    "        # learnrate (float): the learning rate (between 0.0 and 0.1).\n",
    "        # n_iter (int): the number of iterations over the training data.\n",
    "\n",
    "        self.learnrate = learnrate\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    # Define fit function.\n",
    "    def fit(self, X, y):\n",
    "        # X (array-like, shape = [n_observations, n_features]): the training data in the form of vectors with\n",
    "        #     n_observations observations and n_features features.\n",
    "        # y (array-like, shape [n_observations]): the vector of output values (labels).\n",
    "\n",
    "        # Initialize the weights vector and error-count list.\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "            # w_ (1d-array): the weights vector after fitting the current observation and updating the weights\n",
    "            # errors_ (list): the number of misclassifications in the current epoch.\n",
    "\n",
    "        # For each iteration, update the weights vector and register a misclassification if necessary.\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "                # errors (int): counter for misclassifications.\n",
    "            for xi, target in zip(X,y):\n",
    "                update = self.learnrate * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    # Define a \"net input\" function.\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    # Define a predict function.\n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Walkthrough ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a detailed, step-by-step explanation of the Python implementation of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model class. ###\n",
    "\n",
    "* **Define the initializer.**\n",
    "\n",
    "\n",
    "* **Define a 'fit' function** that passes over each observation in the training data set, updates the weights vector each time, and tracks the number of misclassifications of observations.\n",
    "\n",
    "    * Initialize the weights vector and error-count list (number of misclassifications per epoch (i.e., one full pass of the ML algorithm over the whole of the training data). By convention, use names that end in `_` for attributes of the Perceptron class that are not part of the initialization.\n",
    "    \n",
    "        * `zeros` creates an array of all zeros. `X.shape` returns a tuple of form `(n,m)` where `n` is the number of rows of `X` and `m` is the number of columns. So, `X.shape[1]` returns the second value of the tuple, namely the number of columns. We add `1` to that number in order to create a zeros vector with one additional value, which will represent `w_0`, the 1st coefficient (which will ultimately equal the negative of our threshold). We will also add a `1` to the beginning of the features vector. This essentially allows us to draw the class divider line at `0`, rather than at our threshold value, which helps simplify things. For a detailed explanation of the math behind this algorithm, see: https://en.wikipedia.org/wiki/Perceptron.\n",
    "    \n",
    "        * So, `np.zeros(1 + Z.shape[1])` returns an `1d-array` of all zeros of length `1 + X.shape[1]`. We also create an empty array `errors_` to track the number of misclassifications.\n",
    "    \n",
    "    * For each iteration, update the weights vector and register a misclassification if necessary.\n",
    "        * For each observation, `xi`, and output value (predicted label), `target`, carry out the following steps. Note that `zip(X,y)` returns a tuple comprised of the first row (observation) in the array `X` and the corresponding output value in `y` (the ground truth value).\n",
    "            * Define a new variable, `update`, and set it equal to the learning rate multiplied by the difference between `target`, the ground truth label value, and `self.predict(xi)`, the predicted label value for observation `xi`. Note, we define the `predict()` function below.\n",
    "            * Add the updated weight amounts to each weight in the weights vector, aside from `w_0`.\n",
    "            * Since, `w_0` is associated with the `1` that occupies the first position of the features vector, we don't multiply it by anything.\n",
    "            * Track misclassification in `errors`, if necessary. If `update == 0.0`, the classification was correct. Otherwise, it was not. Note, `int(True)` returns `1` and `int(False)` returns `0`.\n",
    "        * Update the list of misclassifications, `self.errors_`.\n",
    "    \n",
    "    \n",
    "* **Define a 'net input' function.**\n",
    "    \n",
    "    * This is simply the dot product of the weights vector and the features vector. We will feed this into our \"activation function\" to see whether the observation is of the class in question, or is not. Note, the term \"activation function\" comes from the original Perceptron Model, which was meant to model a single neuron in the human brain. Given an input signal whose strength exceeds a particular threshold, the neuron will activate. If the signal is weaker than the threshold, the neuron will not fire. In our case, the neuron firing happens when the observation falls into the class, and the neuron does not fire when it does not. Note: `X` here is not to be confused with `X` (the `2d array`) in the `fit()` function.\n",
    "    \n",
    "\n",
    "* **Define a 'predict' function.**\n",
    "    * This is the activation function referenced above. It returns either `1` if the observation is predicted to be of the class, and `-1` if the observation is predicted to not be of the class. Note: `X` here is not to be confused with `X` (the `2d array`) in the `fit()` function.\n",
    "        * More specifically, `predict()` returns `1` if `net input()` returns a value greater than or equal to `0.0` (which we said is our \"adjusted\" threshold), and `-1` if `net input()` returns a value less than `0.0`. When the first argument of `np.where` is `True`, the second argument is returned and when the first argument is `False` the third argument is returned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
